{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download the NLTK Punkt tokenizer\n",
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/dstone/.config/autogpt/credentials.yml', 'r') as f:\n",
    "    yml = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up the OpenAI API client\n",
    "openai.api_key = yml['openai']['api_key']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defintions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_embeddings(prompt, model=\"text-embedding-ada-002\"):\n",
    "#     completions = openai.Completion.create(\n",
    "#         engine=model,\n",
    "#         prompt=prompt,\n",
    "#         n=1,\n",
    "#         max_tokens=3,\n",
    "#         temperature=0,\n",
    "#     )\n",
    "\n",
    "#     # Extract the embeddings\n",
    "#     embeddings = completions[\"choices\"][0][\"metadata\"][\"model\"][\"embedding\"]\n",
    "\n",
    "#     return np.array(embeddings)\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\") -> np.array:\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   emb = openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "   return np.asarray(emb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_semantic_index(paragraphs):\n",
    "    semantic_index = []\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        # Obtain the paragraph embeddings\n",
    "        embedding = get_embedding(paragraph)\n",
    "\n",
    "        # Add the paragraph and its embedding to the index\n",
    "        semantic_index.append((paragraph, embedding))\n",
    "\n",
    "    return semantic_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_paragraphs(question, semantic_index, top_n=3):\n",
    "    question_embedding = get_embedding(question)\n",
    "\n",
    "    # Calculate the cosine similarity between the question and paragraph embeddings\n",
    "    similarities = [cosine_similarity(question_embedding.reshape(1, -1), emb.reshape(1, -1)) for _, emb in semantic_index]\n",
    "\n",
    "    # Get the indices of the top_n most relevant paragraphs\n",
    "    top_indices = np.argsort(similarities, axis=0)[::-1][:top_n].flatten()\n",
    "\n",
    "    # Return the most relevant paragraphs\n",
    "    return [semantic_index[i][0] for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_engine = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_context(question, relevant_paragraphs, model=\"gpt-3.5-turbo\", tokens_limit=200, previous_questions: list=[], previous_responses: list=[]):\n",
    "    context = \"\\n\".join(relevant_paragraphs)\n",
    "    prompt = f\"\\n\\nQ: {question}\\nA: \"\n",
    "\n",
    "    # TODO: add in previous context\n",
    "    # example (from https://platform.openai.com/docs/guides/chat/introduction)\n",
    "    # messages=[\n",
    "    #     {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    #     {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"},\n",
    "    #     {\"role\": \"assistant\", \"content\": \"The Los Angeles Dodgers won the World Series in 2020.\"},\n",
    "    #     {\"role\": \"user\", \"content\": \"Where was it played?\"}\n",
    "    # ]\n",
    "    # you can fill in the assistant with previous questions and answers to continue the conversation\n",
    "    # messages = []\n",
    "    # if len(previous_responses) > 0:\n",
    "    #     pass\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[{'role': 'system', 'content': context},\n",
    "                  {'role': 'user', 'content': prompt}],\n",
    "        max_tokens=tokens_limit,\n",
    "        temperature=0.5, # make as deterministic as possible; I just want a number\n",
    "    )\n",
    "\n",
    "    answer = response['choices'][0]['message']['content']\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            text += reader.pages[page_num].extract_text()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load texts and embed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from glob import glob\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_directory = '/Users/dstone/Dropbox/papers/space/terraforming/'\n",
    "\n",
    "pdf_texts = []\n",
    "for file_name in glob(f'{pdf_directory.rstrip(\"/\")}/*.pdf'):\n",
    "    pdf_path = Path(file_name).absolute()\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "    pdf_texts.append(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paragraphs = []\n",
    "for pdf_text in pdf_texts:\n",
    "    paragraphs = nltk.tokenize.sent_tokenize(pdf_text)\n",
    "    all_paragraphs.extend(paragraphs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes time-- calling OpenAI API for every tokenized sentence in texts\n",
    "semantic_index = build_semantic_index(all_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What can you summarize about the role of Nitrogen in terraforming Mars?\"\n",
    "rel_paragraphs = find_relevant_paragraphs(question=question, semantic_index=semantic_index, top_n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ask_with_context(question=question, relevant_paragraphs=rel_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nitrogen is a key factor in the feasibility of terraforming Mars for human habitability.\n",
      "The amount of nitrogen needed to create a breathable atmosphere on Mars is very large, and it is unlikely that the current atmospheric nitrogen is enough.\n",
      "It is possible that nitrogen is tied up as nitrate in the regolith and subsurface, but more research is needed to assess the amount and location of nitrates on Mars.\n",
      "Without enough nitrogen, it is not within near-term capabilities of humans to bring it to Mars.\n",
      "However, nitrogen is unlikely to be limiting for a plant-dominated biosphere.\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(nltk.tokenize.sent_tokenize(response)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = get_embedding(question)\n",
    "\n",
    "# Calculate the cosine similarity between the question and paragraph embeddings\n",
    "similarities = [cosine_similarity(question_embedding.reshape(1, -1), emb.reshape(1, -1)) for _, emb in semantic_index]\n",
    "\n",
    "# Get the indices of the top_n most relevant paragraphs\n",
    "top_indices = np.argsort(similarities, axis=0)[::-1][:3].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77780824]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(question_embedding.reshape(1, -1), semantic_index[0][1].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.,  1.,  1., ..., -1., -1., -1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarities[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYour question here\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Find the most relevant paragraphs for the question\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m relevant_paragraphs \u001b[39m=\u001b[39m find_relevant_paragraphs(question, semantic_index)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Ask the question with the relevant context\u001b[39;00m\n\u001b[1;32m      7\u001b[0m answer \u001b[39m=\u001b[39m ask_with_context(question, relevant_paragraphs)\n",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m, in \u001b[0;36mfind_relevant_paragraphs\u001b[0;34m(question, semantic_index, top_n)\u001b[0m\n\u001b[1;32m      2\u001b[0m question_embedding \u001b[39m=\u001b[39m get_embedding(question)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Calculate the cosine similarity between the question and paragraph embeddings\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m similarities \u001b[39m=\u001b[39m [cosine_similarity(question_embedding\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), emb\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)) \u001b[39mfor\u001b[39;49;00m _, emb \u001b[39min\u001b[39;49;00m semantic_index]\n\u001b[1;32m      7\u001b[0m \u001b[39m# Get the indices of the top_n most relevant paragraphs\u001b[39;00m\n\u001b[1;32m      8\u001b[0m top_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(similarities, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:top_n]\u001b[39m.\u001b[39mflatten()\n",
      "Cell \u001b[0;32mIn[24], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m question_embedding \u001b[39m=\u001b[39m get_embedding(question)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Calculate the cosine similarity between the question and paragraph embeddings\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m similarities \u001b[39m=\u001b[39m [cosine_similarity(question_embedding\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), emb\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39mfor\u001b[39;00m _, emb \u001b[39min\u001b[39;00m semantic_index]\n\u001b[1;32m      7\u001b[0m \u001b[39m# Get the indices of the top_n most relevant paragraphs\u001b[39;00m\n\u001b[1;32m      8\u001b[0m top_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margsort(similarities, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][:top_n]\u001b[39m.\u001b[39mflatten()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "question = \"Your question here\"\n",
    "\n",
    "# Find the most relevant paragraphs for the question\n",
    "relevant_paragraphs = find_relevant_paragraphs(question, semantic_index)\n",
    "\n",
    "# Ask the question with the relevant context\n",
    "answer = ask_with_context(question, relevant_paragraphs)\n",
    "print(f\"Q: {question}\\nA: {answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nYour large text corpus goes here.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt_playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
